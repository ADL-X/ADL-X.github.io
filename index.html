<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
 

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LLAVIDAL</title>
  <link rel="icon" type="image/x-icon" href="static/images/llavidal_logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://chakrabortyrajatsubhra.github.io/" target="_blank">Rajatsubhra Chakraborty</a><sup>*,&#9830;</sup>,
              </span>
              <span class="author-block">
                <a href="https://webpages.charlotte.edu/asinha13/" target="_blank">Arkaprava Sinha</a><sup>*,&#9830;</sup>,
              </span>
              <span class="author-block">
                  <a href="https://dominickrei.github.io/" target="_blank">Dominick Reilly</a><sup>*,&#9830;</sup>,
              </span>
              <span class="author-block">
                <a href="https://manishgovind.github.io/" target="_blank">Manish Kumar Govind</a><sup>&#9830;</sup>,
              </span>
              <br>
                    <span class="author-block">
                      <a href="https://webpages.charlotte.edu/pwang13/" target="_blank">Pu Wang</a><sup>&#9830;</sup>,</span>
                      <span class="author-block">
                        <a href="https://www-sop.inria.fr/stars/Francois.Bremond/" target="_blank">Francois Bremond</a><sup> &#8224;</sup>,
                      </span>
                      <span class="author-block">
                        <a href="https://srijandas07.github.io/" target="_blank">Srijan Das</a><sup>&#9830;</sup></span>

                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"> &#9830; UNC Charlotte ,&#x2020; Inria,&#x2020; Université Côte d'Azur  <br>  </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                       <!-- ArXiv abstract Link -->
                       <!-- <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.09390" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a> -->
                    </span> 
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2406.09390" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>paper</span>
                      </a>
                    </span>
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ADL-X/LLAVIDAL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!--Data Gdrive Link -->
                <span class="link-block">
                  <a href="https://tinyurl.com/llavidal-features" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-database"></i>
                      </span>
                      <span>Data</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/adlxteaser_final.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Comparison of LLVM vs LLAVIDAL : In real world scenarios, web-video trained models
struggle to understand Activities of Daily Living due to the subtle nuances in the video, whereas our
ADL-X trained LLAVIDAL model triumphs in understanding complex human-object interactions.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Vision Models (LLVMs) have demonstrated effectiveness in processing internet videos, yet they struggle with the visually perplexing dynamics present in Activities of Daily Living (ADL) due to limited pertinent datasets and models tailored to relevant cues. 
            To this end, we propose a framework for curating ADL multiview datasets to fine-tune LLVMs, resulting in the creation of ADL-X, comprising 100K RGB video-instruction pairs, language descriptions, 3D skeletons, and action-conditioned object trajectories. We introduce LLAVIDAL, an LLVM capable of incorporating 3D poses and relevant object trajectories to understand the intricate spatiotemporal relationships within ADLs. Furthermore, we present a novel benchmark, ADLMCQ, for quantifying LLVM effectiveness in ADL scenarios. When trained on ADL-X, LLAVIDAL consistently achieves state-of-the-art performance across all ADL evaluation metrics. Qualitative analysis reveals LLAVIDAL's temporal reasoning capabilities in understanding ADL.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/ADL-Data curation process.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          ADL-X Data Curation Process
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ADL-architecture.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          LLAVIDAL Architecture
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="hero is-small">
  
  <div class="container">
    <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
    <div class="results-grid">
      
      <div class="item">
        <img src="static/images/Impact_of_ADL-X_Training.png" alt="Quantitative Result 7">
        <h2 class="subtitle">Impact of ADL-X Training</h2>
      </div>
      <div class="item">
        <img src="static/images/charades_SH.png" alt="Quantitative Result 2">
        <h2 class="subtitle">ADLMCQ - Action Recognition</h2>
      </div>
      <div class="item">
        <img src="static/images/lemma_TSU.png" alt="Quantitative Result 3">
        <h2 class="subtitle">ADLMCQ - Action Forecasting</h2>
      </div>
      <div class="item">
        <img src="static/images/ADLMCQ-AR.png" alt="Quantitative Result 4">
        <h2 class="subtitle">Effect of Introduction of Pose and Object Cues on ADLMCQ Action Recognition</h2>
      </div>
      <div class="item">
        <img src="static/images/ADLMCQ-AF.png" alt="Quantitative Result 5">
        <h2 class="subtitle">Effect of Introduction of Pose and Object Cues on ADLMCQ Action Forecasting</h2>
      </div>
      <div class="item">
        <img src="static/images/AD.png" alt="Quantitative Result 6">
        <h2 class="subtitle">Effect of Introduction of Pose and Object Cues on ADLMCQ Action Description</h2>
      </div>
      
    </div>
  </div>
</section>


<section class="hero is-small">
  
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/VD.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
          Video Description
          </h2>
        </div>
       <div class="item">
        <!-- Your image here -->

        <img src="static/images/AR.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Action Recognition
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/AF.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        Action Forecasting
        </h2>
      </div>
  </div>
</div>
</div>
</section>








<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title has-text-centered">BibTeX</h2>
      <pre><code>
        @misc{chakraborty2024llavidal,
          title={LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living}, 
          author={Rajatsubhra Chakraborty and Arkaprava Sinha and Dominick Reilly and Manish Kumar Govind and Pu Wang and Francois Bremond and Srijan Das},
          year={2024},
          eprint={2406.09390},
          archivePrefix={arXiv},
          primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
        } 
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!--License citation-->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title has-text-centered">Usage License</h2>
    <p>The dataset is protected under the CC-BY license of Creative Commons, which allows users to distribute, remix, adapt, and build upon the material in any
      medium or format, as long as the creator is attributed. The license allows ADL-X for commercial use. As the authors of this manuscript and collectors of this dataset, we reserve the right to distribute
      the data.</p>
  </div>
</section>
<!--End License-->




  <footer class="footer">
  
    <div class="container is-max-desktop content">
          <h2 class="title has-text-centered">Acknowledgement</h2>
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template
            </a>.
          </p>

  
  </div>
</footer>
  </body>
  </html>
